{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16aa309f",
   "metadata": {},
   "source": [
    "Before analysing the data we have to do some - more - datawrangling. We have to take our two dataframes of interest, and somehow merge them into one timedomain dataframe. We then have to import this combined dataframe into MNE - making it understandable for this toolbox by creating whats called 'event_arrays'.\n",
    "\n",
    "We assume to have a `stim_df` (in stimulus domain - one row per stimulation) and a `data_df` (in time domain). The stim_df should have all the regressors we want to use in, whereass the data_df should have all predicted ('y'-timecourse) data in it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17880567",
   "metadata": {},
   "source": [
    "### Step 1. merging dataframes into timedomain data\n",
    "As a first step we have to map the `stim_df` into the timedomain data, going from a structure where we have one row per stimulus to a structure where we have many timepoints per stimulus.\n",
    "\n",
    "Here we assume to have collumn in both the `stim_df` and the `data_df` with clock information (that match, see at the end of this notebook for more information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fe99d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MOVE FUNCTIONS FROM STIMULUS DOMAIN INTO TIMEDOMAIN\n",
    "## MAP 'STIMULUS' TO INDEX OF WHAT STIMULI, FOR EASY MAPPING\n",
    "\n",
    "def stim_save_segments(df_beh, groupby_nm=['block', 'segment']):\n",
    "    \"\"\"apply a new column to behavioural dataframe with segment_all - \n",
    "    indicating a continious numerical indicator of what segment we are on\"\"\"\n",
    "\n",
    "    # predefine segement all in df_beh\n",
    "    df_beh['segment_all'] = np.nan\n",
    "    \n",
    "    # loop over block and segment combinations\n",
    "    for idx, row in df_beh.groupby(groupby_nm).first().reset_index().iterrows():\n",
    "\n",
    "        # save new segment all \n",
    "        df_beh.loc[(df_beh['block'] == row['block']) & (df_beh['segment'] == row['segment']), 'segment_all'] = idx \n",
    "\n",
    "    # return the dataframe\n",
    "    return(df_beh)\n",
    "\n",
    "\n",
    "def map_stim_to_time(df, df_beh, cn_stim='stimulus', cn_ts='TIMESTAMP', \n",
    "                     beh_cn_onset='timing_meg', beh_cn_offset='timing_offset_meg'):\n",
    "    \"\"\"transform / map stimulus dataframe into the time domain\n",
    "    input: df: Pandas dataframe - time domain\n",
    "           df_beh: Pandas dataframe - stim domain\n",
    "           cn_stim: (optional) column name for stimulus indicator\n",
    "           cn_ts: (optional) column name for timestamp indicater in time df\n",
    "           beh_cn_onset: (optional) column name for onset time in beh_df\n",
    "           beh_cn_offset: (optional) column name for offset time in beh_df\"\"\"\n",
    "    \n",
    "    # Apply the function to create the 'stimulus' column in df_time\n",
    "    df['stimulus'] = df['TIMESTAMP'].apply(_assign_stimulus, \n",
    "                                           df_beh=df_beh,\n",
    "                                           cn_onset=beh_cn_onset,\n",
    "                                           cn_offset=beh_cn_offset,\n",
    "                                           cn_stimulus=cn_stim)\n",
    "    # Return the dataframe\n",
    "    return(df)\n",
    "\n",
    "\n",
    "def map_columns_to_time(df, df_beh, col_to_trans, indicator_nm='stimulus'):\n",
    "    \"\"\"map columns of interest to transfer to timedomain\n",
    "    df: dataframe in timedomain\n",
    "    df_beh: dataframe in stim domain\n",
    "    col_to_trans: all columns to transfer\n",
    "    indicator_nm: (optional) indicator name - what to use for the mapping\"\"\"\n",
    "    \n",
    "    # loop over columns to transfer\n",
    "    for colnm in col_to_trans:\n",
    "\n",
    "        # create a dictionary to map 'stimulus' to all conditions I want to transfer to the other df\n",
    "        stimulus_to_col = df_beh.set_index(indicator_nm)[colnm].to_dict()\n",
    "\n",
    "        # map function to go from one to another\n",
    "        df[colnm] = df[indicator_nm].map(stimulus_to_col).fillna(0)\n",
    "        \n",
    "    # returns dataframe\n",
    "    return(df)\n",
    "\n",
    "\n",
    "def map_block_to_run(df, df_beh, run_nm='run', block_nm_beh='block', block_nm='BLOCK'):\n",
    "    \"\"\"map from blocknumber to run number in timedomain\"\"\"\n",
    "\n",
    "    # create a dictionary to map 'block' to run\n",
    "    map_block_run = df_beh.set_index(block_nm_beh)[run_nm].to_dict()\n",
    "\n",
    "    # map the df\n",
    "    df[run_nm] = df[block_nm].map(map_block_run).fillna(0).astype(int)\n",
    "\n",
    "    # return dataframe\n",
    "    return(df)\n",
    "\n",
    "\n",
    "def time_save_segments(df, df_beh,\n",
    "                       groupby_nm=['block', 'segment'],\n",
    "                       onset_nm='timing_meg',\n",
    "                       offset_nm='timing_offset_meg',\n",
    "                       timing_mm='TIMESTAMP'\n",
    "                      ):\n",
    "    \"\"\"save segments into the time domain dataframe\n",
    "    df: timedomain dataframe\n",
    "    df_beh: stimdomain dataframe\n",
    "    groupby_nm: (optional) list of names to groupby\n",
    "    onset_nm: (optional) what to use as onset timings - in same timeframe\n",
    "    offset_nm: (optional) what to use as offset timings - in same timeframe\n",
    "    timing_nm: (optional) time indicator in original dataframe\"\"\"\n",
    "\n",
    "    # get dataframe of onset and offset timings only\n",
    "    onset_df = df_beh.groupby(groupby_nm).first()[onset_nm].reset_index()\n",
    "    offset_df = df_beh.groupby(groupby_nm).last()[offset_nm].reset_index()\n",
    "\n",
    "    # predefine all new columns in our timedomain dataframe\n",
    "    df['block'] = np.nan\n",
    "    df['segment'] = np.nan\n",
    "    df['segment_all'] = np.nan\n",
    "\n",
    "    # loop over all index (combinations)\n",
    "    for idx, row in onset_df.iterrows():\n",
    "\n",
    "        # get start and endtime of groupby section\n",
    "        cur_onset = onset_df[onset_nm].iloc[idx]\n",
    "        cur_offset = offset_df[offset_nm].iloc[idx]\n",
    "\n",
    "        # map to OG dataframe\n",
    "        df.loc[(df[timing_mm] >= cur_onset) & (df[timing_mm] <= cur_offset), 'segment'] = onset_df['segment'].iloc[idx]\n",
    "\n",
    "        # save per segment indicator\n",
    "        df.loc[(df[timing_mm] >= cur_onset) & (df[timing_mm] <= cur_offset), 'segment_all'] = idx\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "\n",
    "def time_save_onoff(df, onoff_nm='onoff', indicator='stimulus'):\n",
    "    \"\"\"save onoff value (bool), based on indicator value\"\"\"\n",
    "\n",
    "    # predefine\n",
    "    df['onoff'] = 0\n",
    "    # take wherever there is any stimulus - set to 1\n",
    "    df.loc[(df['stimulus'] > 0), 'onoff'] = 1\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "\n",
    "# create a function to assign stimuli based on timing\n",
    "def _assign_stimulus(timing, \n",
    "                     df_beh, \n",
    "                     cn_onset='timing_meg', \n",
    "                     cn_offset='timing_offset_meg',\n",
    "                     cn_stimulus='stimulus'):\n",
    "    \"\"\"pandas apply function to get stimuli into the time domain\n",
    "    input df_beh, cn_onset (optional columnname of onset time),\n",
    "    cn_offset (optional columnname of offset time), cn_stimulus (optional columnname of stimulus)\"\"\"\n",
    "    idx = np.searchsorted(df_beh[cn_onset], timing)\n",
    "    if idx == 0 or timing >= df_beh[cn_offset].iloc[idx - 1]:\n",
    "        return 0\n",
    "    return df_beh[cn_stimulus].iloc[idx - 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267bdca6",
   "metadata": {},
   "source": [
    "We use these functions in two discrete steps: first to create a mapping of each stimulus from the stimulus dataframe into the timingbased dataframe. And second to map collumns of interest into the new dataframe\n",
    "\n",
    "`map_stim_to_time()` Example:\n",
    "\n",
    "\"stim_df\"\n",
    "| 'stim_nr' | 'timing' | 'timing_offset' |\n",
    "| --- | --- | --- |\n",
    "| 1 | 1.24 | 1.248 |\n",
    "| 2 | 1.25 | 1.258 |\n",
    "| 3 | 1.26 | 1.268 |\n",
    "| 4 | 1.27 | 1.278 |\n",
    "\n",
    "\"data_df\" returned\n",
    "| 'old_columns' | 'stim_nr' | 'timing' |\n",
    "| --- | --- | --- | \n",
    "| ... | 0 | 1.238 |\n",
    "| ... | 1 | 1.240 |\n",
    "| ... | 1 | 1.242 |\n",
    "| ... | 1 | 1.244 |\n",
    "| ... | 1 | 1.246 |\n",
    "| ... | 0 | 1.248 |\n",
    "| ... | 2 | 1.250 |\n",
    "| ... | 2 | 1.252 |\n",
    "| ... | 2 | 1.254 |\n",
    "| ... | 2 | 1.256 |\n",
    "| ... | 2 | 1.258 |\n",
    "| ... | 0 | 1.260 |\n",
    "| ... | ... | ... |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ae503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map stimulus indexing to timedomain (123 > 00011100222000333)\n",
    "df = map_stim_to_time(df, df_beh, cn_stim='stimulus', cn_ts='TIMESTAMP', \n",
    "                     beh_cn_onset='timing_meg', beh_cn_offset='timing_offset_meg')\n",
    "\n",
    "# get columns of interest to transfer and apply stim specific mapping\n",
    "col_to_trans = ['surprisal_a', 'surprisal_b']\n",
    "df = map_columns_to_time(df, df_beh, col_to_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e97fd6d",
   "metadata": {},
   "source": [
    "Additionally, we can do the same in different domains, so for example per segment, block, miniblock, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8f1543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the blocknumber to runnumber pairing in stimulus domain to map block to run in time domain\n",
    "df = map_block_to_run(df, df_beh)\n",
    "\n",
    "# map segment specific data onto current segment\n",
    "col_to_trans = ['center_freq_a', 'center_freq_b', 'center_freq_a_oct', 'center_freq_b_oct', 'probability_a', 'probability_b']\n",
    "df = map_columns_to_time(df, df_beh, col_to_trans, indicator_nm='segment_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85c6959",
   "metadata": {},
   "source": [
    "### Step 1b. Impulse mapping\n",
    "For now we mapped the full duration of onset and offset in the following manner: (123 > 00011100222000333). However, sometimes we just want to know the onset of a stimulus, or in other words; the impulse. We can map impulses using `time_transform_FIR()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4db39f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_transform_FIR(df, indicator, fir_columns,\n",
    "                       prefix_fir = 'FIR_',\n",
    "                       prefix_fir_offset = 'OFFSET_FIR_'):\n",
    "    \"\"\"within a dataframe, groupby indicator > loop over fir columns and make a boxplot a impulse (start + end)\n",
    "    input: df: timedomain dataframe\n",
    "           indicator: indicator for groupby (unqiue indicator that binds the fir_columns)\n",
    "           fir_columns: the fir columns to loop over and add\n",
    "           prefix_fir: (optional) prefix naming for new fir naming\n",
    "           prefix_fir_offset: (optional) prefix naming for new fir ofset naming\n",
    "    return: return adjusted dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # create onset and ofset arrays for FIR modelling - per stimulus\n",
    "    onset_idx  = df[df[indicator] > 0].groupby(indicator).apply(lambda x: x.index[0]).to_numpy()  # onset\n",
    "    offset_idx = df[df[indicator] > 0].groupby(indicator).apply(lambda x: x.index[-1]).to_numpy() # offset\n",
    "\n",
    "    # loop over columns of interest to re-insert into df as FIR\n",
    "    for col in fir_columns:\n",
    "\n",
    "        # predefine columns\n",
    "        df[f'{prefix_fir}{col}'] = 0\n",
    "        df[f'{prefix_fir_offset}{col}'] = 0\n",
    "\n",
    "        # take first or last value from boxplot and make FIR\n",
    "        df.loc[onset_idx, f'{prefix_fir}{col}'] = df.loc[onset_idx, col]\n",
    "        df.loc[offset_idx, f'{prefix_fir_offset}{col}'] = df.loc[offset_idx, col]\n",
    "        \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac826604",
   "metadata": {},
   "source": [
    "and can use it as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c3615e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get columns we want to take first instanse impulse for \n",
    "fir_columns  =  ['surprisal_a', 'surprisal_b']\n",
    "\n",
    "# add fir impulses to dataframe\n",
    "df = time_transform_FIR(df, 'stimulus', fir_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b5e312",
   "metadata": {},
   "source": [
    "### Step 2. Generate event dataframes\n",
    "MNE uses event dataframes (https://mne.tools/dev/auto_tutorials/raw/20_event_arrays.html), so yet another way of coding onsets and offsets. Events have a shape of [nr_events, 3], where the 0th column are the indexes (sample number in timedomain), and the 2th column are the values (magnitude). The middle column (1st column) is a value indicating what the event code was on the immediately preceding sample. In practice, that value is almost always 0, but it can be used to detect the endpoint of an event whose duration is longer than one sample.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53ee44e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_event_from_df(raw, df, event_nm):\n",
    "    \"\"\"within the raw MNE object, using the matching nonzero indexes in df to add mne events\n",
    "    input: raw: mne raw object - must include 'sfreq', here we add stim to\n",
    "            df: dataframe with our stimulus (impulse/block) information\n",
    "            event_nm: the name of the column in the dataframe which is the event\n",
    "    output: returns raw mne object with object\"\"\"\n",
    "\n",
    "    # calulate index position of events and the value of those events\n",
    "    idxs      = np.where(df[event_nm] > 0)\n",
    "    value     = df[event_nm].to_numpy()[idxs]\n",
    "\n",
    "    # predefine numpy array in correct shape for mne\n",
    "    mne_arr = np.zeros((len(idxs[0]), 3))\n",
    "    mne_arr[:,0] = idxs[0]\n",
    "    mne_arr[:,2] = value\n",
    "\n",
    "    # # create new stimulus channel - ONOFF\n",
    "    if event_nm not in raw.ch_names:\n",
    "        temp_info = mne.create_info([event_nm], raw.info['sfreq'], ['stim'])\n",
    "        stim_raw = mne.io.RawArray(df[[event_nm]].to_numpy().T, temp_info)\n",
    "        raw.add_channels([stim_raw], force_update_info=True)\n",
    "\n",
    "    # add actuall events\n",
    "    raw.add_events(mne_arr, stim_channel=event_nm, replace=True)\n",
    "    return(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1771c2a9",
   "metadata": {},
   "source": [
    "We can simply add them by sellecting the column of intest. This approach allows us the retrieve events using the inbuild MNE function `mne.find_events(raw, stim_channel='FIR_onoff')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f2fa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_columns = ['FIR_surprisal_a',\n",
    "                'FIR_surprisal_b',\n",
    "                'FIR_onoff',\n",
    "                'onoff']\n",
    "\n",
    "# add events for all \n",
    "for evnt in event_columns:\n",
    "    raw = add_event_from_df(raw, df, evnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a56270",
   "metadata": {},
   "source": [
    "### Step 3. Doing testing within MNE\n",
    "Testing in MNE is fairly straighforward if we want to do simple linear regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b3edf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from mne.stats import linear_regression\n",
    "\n",
    "# Find events\n",
    "events = mne.find_events(raw, stim_channel='FIR_onoff')\n",
    "\n",
    "# Create epochs ('timelocked data' per 'event')\n",
    "epochs = mne.Epochs(raw, events, tmin=-0.1, tmax=0.5, preload=True)\n",
    "\n",
    "# Get the data for the 'fir_surprisal' channel\n",
    "fir_surprisal_data = raw.copy().pick_channels(['FIR_onoff']).get_data()\n",
    "\n",
    "# Perform linear regression - in this case for on-off\n",
    "res = linear_regression(epochs, fir_surprisal_data[fir_surprisal_data > 0, np.newaxis], names=['FIR_onoff'])\n",
    "\n",
    "# Access the beta values and related information\n",
    "beta_values = res['FIR_onoff'].beta\n",
    "t_values = res['FIR_onoff'].t_val\n",
    "p_values = res['FIR_onoff'].p_val\n",
    "\n",
    "# Access and plot the beta values directly\n",
    "res['FIR_onoff'].beta.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9a7cd8",
   "metadata": {},
   "source": [
    "Note that you can even do this approach in MNE when you have some other kind of timecourse data instead of EEG or MEG data (for example pupil responses). We just create some dummy info and plug in our timecourse data. You may want to adjust this dummy information function to suite your need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd8a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_info():\n",
    "    # Step 1: Read the layout\n",
    "    layout = mne.channels.read_layout('CTF275.lay')\n",
    "\n",
    "    # Step 2: Create channel information\n",
    "    # Assuming all channels are MEG channels\n",
    "    ch_names = layout.names\n",
    "    ch_types = ['mag'] * len(ch_names)  # Adjust if you have different types of channels\n",
    "\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=60, ch_types=ch_types)\n",
    "    info._unlocked = True\n",
    "    info['highpass'] = 0.0\n",
    "    info['lowpass'] = 600.0\n",
    "    return(info)\n",
    "\n",
    "# create dummy information\n",
    "info = create_dummy_info()\n",
    "\n",
    "# create evoked mne data from whatever data we want\n",
    "evoked = mne.EvokedArray(data.T, \n",
    "                         info,                         #dummy info\n",
    "                         tmin=MNE_evokeds[0][0].tmin,  #the t=0-point\n",
    "                         baseline=(-0.02, 0.02))       #demean over 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af7a8b7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "621bffa0",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d7c50e",
   "metadata": {},
   "source": [
    "note on timing: since the MEG system and the stimulus pc both CLOCK stimuli seperately the clocks have to be synced. We can do so as long as we have a known point in both modelities - which will give us a `sync_val` (a value denoting the difference between one and the other timing).\n",
    "\n",
    "we can then simply: add this delay to get the timing in the other modality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94093dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_timing(df, sync_val, timingname='timing', new_timingname='timing_meg',\n",
    "                              timingname_offset='timing_offset', new_timingname_offset='timing_offset_meg'):\n",
    "    \"\"\"use syncing value to get timings from stimpc domain into the MEG clock domain\n",
    "    input df and sync value, returns adjusted dataframe\"\"\"\n",
    "\n",
    "    # create new column in old dataframe\n",
    "    df[new_timingname] = df[timingname] + sync_val\n",
    "    df[new_timingname_offset] = df[timingname_offset] + sync_val\n",
    "    # and return\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a196c525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
