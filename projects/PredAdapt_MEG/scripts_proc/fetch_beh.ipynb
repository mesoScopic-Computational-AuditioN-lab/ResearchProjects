{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70d30e89",
   "metadata": {},
   "source": [
    "## Loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08a564eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/jorvhar/50832131.dccn-l029.dccn.nl/ipykernel_18863/3868052171.py:23: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import scipy.io\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "import contextlib\n",
    "from copy import deepcopy\n",
    "import imp \n",
    "import time \n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215c2da0",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e32cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "pp = 1\n",
    "task = 0     # 0: main, 1: localizer\n",
    "loc_run = 1  # run 1 or run 2\n",
    "\n",
    "# MEG directory and file names\n",
    "tasks = ['main', 'loc']\n",
    "input_dir = f'/project/3018063.01/preproc/sub-{pp:03}/preproc/{tasks[task]}/'\n",
    "out_fn = f'{tasks[task]}_behdf_sub-{pp:03}.csv'\n",
    "\n",
    "# set behevioural directiories\n",
    "stim_dir = f'/project/3018063.01/beh/stimuli/{pp}'\n",
    "loud_dir = f'/project/3018063.01/beh/loudness/{pp}'\n",
    "data_dir = f'/project/3018063.01/beh/data/{pp}'\n",
    "\n",
    "#fns\n",
    "sync_fn = f'MEG_sync_sub-{pp:03d}.mat'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1dff4e",
   "metadata": {},
   "source": [
    "# loading log data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0669bfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main loading\n",
    "def data_load(pp,data_dir, stim_dir):\n",
    "    \"\"\"load mainpred mat file and stimuli matfile\"\"\"\n",
    "    mat = scipy.io.loadmat(join(data_dir,\n",
    "                                f'{pp}-mainpred.mat'))\n",
    "    stimuli = scipy.io.loadmat(join(stim_dir, \n",
    "                                    f'{pp}_main_stims.mat'))\n",
    "    return(mat, stimuli)\n",
    "\n",
    "\n",
    "def stims_load(mat, stimuli):\n",
    "    \"\"\"using information from stimuli and pulse timing create dataframe \n",
    "    with frequency information, pulse location etc.\n",
    "    note: 'volume_rel' & 'vol_abs' are the volume where this stimuli was measured\n",
    "    'closest_volume_rel' & 'closest_volume_abs' are the volume which is the closest in time\n",
    "    (half tr shift) - since a tr should capture information within that tr\"\"\"\n",
    "\n",
    "    # set arrays\n",
    "    freqz   = np.array([])\n",
    "    timingz  = np.array([])\n",
    "    timings_offsetz  = np.array([])\n",
    "    runz     = np.array([])\n",
    "    blockz   = np.array([])\n",
    "    segmenz  = np.array([])\n",
    "    centaz   = np.array([])\n",
    "    centbz   = np.array([])\n",
    "    probaz   = np.array([])\n",
    "    probbz   = np.array([])\n",
    "\n",
    "    for blk in np.arange(1, mat['timingz'][1].max()+1):\n",
    "        # get blockidx\n",
    "        idxblock = np.where(mat['timingz'][1] == blk) # where block is 1\n",
    "\n",
    "        #get frequency presentation data for block\n",
    "        frequencies = stimuli['pres_freq'][int(blk)-1, :]\n",
    "\n",
    "        # other values\n",
    "        tps = np.sum(mat['timingz'][3, idxblock] == 1) # get trials per secion\n",
    "\n",
    "        #get timings back from mat file, substract begin time\n",
    "        timings = mat['timingz'][6, idxblock]\n",
    "        timings_offset = mat['timingz'][7, idxblock]\n",
    "        matidx = np.where(mat['segmentz'][1] == blk)\n",
    "\n",
    "        # append to arrays\n",
    "        freqz = np.append(freqz, frequencies)\n",
    "        timingz = np.append(timingz, timings)\n",
    "        timings_offsetz = np.append(timings_offsetz, timings_offset)\n",
    "        runz = np.append(runz, np.repeat(mat['segmentz'][0][matidx], tps))\n",
    "        blockz = np.append(blockz, np.repeat(mat['segmentz'][1][matidx], tps))\n",
    "        segmenz = np.append(segmenz, np.repeat(mat['segmentz'][2][matidx], tps))\n",
    "        centaz = np.append(centaz, 2**np.repeat(mat['segmentz'][7][matidx], tps))   # cent freq a\n",
    "        centbz = np.append(centbz, 2**np.repeat(mat['segmentz'][8][matidx], tps))  # cent freq b\n",
    "        probaz = np.append(probaz, np.repeat(mat['segmentz'][5][matidx], tps))\n",
    "        probbz = np.append(probbz, np.repeat(mat['segmentz'][6][matidx], tps))\n",
    "\n",
    "    # oct variant \n",
    "    freqz_oct = np.log2(freqz)\n",
    "    centaz_oct = np.log2(centaz)\n",
    "    centbz_oct = np.log2(centbz)\n",
    "\n",
    "    # put data into a dictionary and subsequentially in a dataframe\n",
    "    stim_df_dict = {'frequencies': freqz,\n",
    "                    'frequencies_oct': freqz_oct,\n",
    "                    'timing': timingz,\n",
    "                    'timing_offset': timings_offsetz,\n",
    "                    'run': runz,\n",
    "                    'block': blockz,\n",
    "                    'segment': segmenz,\n",
    "                    'center_freq_a': centaz,\n",
    "                    'center_freq_b': centbz,\n",
    "                    'center_freq_a_oct': centaz_oct,\n",
    "                    'center_freq_b_oct': centbz_oct,\n",
    "                    'probability_a': probaz,\n",
    "                    'probability_b': probbz\n",
    "                   }\n",
    "\n",
    "    stim_df = pd.DataFrame(stim_df_dict)\n",
    "    # Add the 'stimulus' column to df_beh\n",
    "    stim_df['stimulus'] = stim_df.index + 1\n",
    "    return(stim_df)\n",
    "\n",
    "\n",
    "def sync_timing(df, sync_val, timingname='timing', new_timingname='timing_meg',\n",
    "                              timingname_offset='timing_offset', new_timingname_offset='timing_offset_meg'):\n",
    "    \"\"\"use syncing value to get timings from stimpc domain into the MEG clock domain\n",
    "    input df and sync value, returns adjusted dataframe\"\"\"\n",
    "\n",
    "    # create new column in old dataframe\n",
    "    df[new_timingname] = df[timingname] + sync_val\n",
    "    df[new_timingname_offset] = df[timingname_offset] + sync_val\n",
    "    # and return\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5d2028e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sync files\n",
    "sync_mat = scipy.io.loadmat(join(data_dir, sync_fn))\n",
    "sync_val = sync_mat['MEG_sync']['mn'][0][0][0,0]\n",
    "\n",
    "# get mat and stimuli struct\n",
    "mat, stimuli = data_load(pp, data_dir, stim_dir)\n",
    "\n",
    "# put in dataframe\n",
    "df_beh = stims_load(mat, stimuli)\n",
    "df_beh = sync_timing(df_beh, sync_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa2b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "\n",
    "for pp in [1, 2, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14]:\n",
    "    task = 0     # 0: main, 1: localizer\n",
    "    loc_run = 1  # run 1 or run 2\n",
    "\n",
    "    # MEG directory and file names\n",
    "    tasks = ['main', 'loc']\n",
    "    input_dir = f'/project/3018063.01/preproc/sub-{pp:03}/preproc/{tasks[task]}/'\n",
    "    out_fn = f'{tasks[task]}_behdf_sub-{pp:03}.csv'\n",
    "\n",
    "    # set behevioural directiories\n",
    "    stim_dir = f'/project/3018063.01/beh/stimuli/{pp}'\n",
    "    loud_dir = f'/project/3018063.01/beh/loudness/{pp}'\n",
    "    data_dir = f'/project/3018063.01/beh/data/{pp}'\n",
    "\n",
    "    #fns\n",
    "    sync_fn = f'MEG_sync_sub-{pp:03d}.mat'\n",
    "\n",
    "    # load sync files\n",
    "    sync_mat = scipy.io.loadmat(join(data_dir, sync_fn))\n",
    "    sync_val = sync_mat['MEG_sync']['mn'][0][0][0,0]\n",
    "\n",
    "    # get mat and stimuli struct\n",
    "    mat, stimuli = data_load(pp, data_dir, stim_dir)\n",
    "\n",
    "    # put in dataframe\n",
    "    df_beh = stims_load(mat, stimuli)\n",
    "    df_beh = sync_timing(df_beh, sync_val)\n",
    "\n",
    "    df_beh.to_csv(join(input_dir, out_fn), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "853f432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df file to csv\n",
    "df_beh.to_csv(join(input_dir, out_fn), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6ae3d7",
   "metadata": {},
   "source": [
    "## loading dataframe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0fb1ca8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'main_behdf_sub-014.csv'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_beh = pd.read_csv(join(input_dir, out_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e5323a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequencies</th>\n",
       "      <th>frequencies_oct</th>\n",
       "      <th>timing</th>\n",
       "      <th>timing_offset</th>\n",
       "      <th>run</th>\n",
       "      <th>block</th>\n",
       "      <th>segment</th>\n",
       "      <th>center_freq_a</th>\n",
       "      <th>center_freq_b</th>\n",
       "      <th>center_freq_a_oct</th>\n",
       "      <th>center_freq_b_oct</th>\n",
       "      <th>probability_a</th>\n",
       "      <th>probability_b</th>\n",
       "      <th>stimulus</th>\n",
       "      <th>timing_meg</th>\n",
       "      <th>timing_offset_meg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>683.438005</td>\n",
       "      <td>9.416667</td>\n",
       "      <td>191025.80758</td>\n",
       "      <td>191026.009564</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>645.854171</td>\n",
       "      <td>1798.010924</td>\n",
       "      <td>9.335065</td>\n",
       "      <td>10.812186</td>\n",
       "      <td>0.891211</td>\n",
       "      <td>0.108789</td>\n",
       "      <td>1</td>\n",
       "      <td>122903.348354</td>\n",
       "      <td>122903.550338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>683.438005</td>\n",
       "      <td>9.416667</td>\n",
       "      <td>191026.05758</td>\n",
       "      <td>191026.259580</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>645.854171</td>\n",
       "      <td>1798.010924</td>\n",
       "      <td>9.335065</td>\n",
       "      <td>10.812186</td>\n",
       "      <td>0.891211</td>\n",
       "      <td>0.108789</td>\n",
       "      <td>2</td>\n",
       "      <td>122903.598354</td>\n",
       "      <td>122903.800354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>683.438005</td>\n",
       "      <td>9.416667</td>\n",
       "      <td>191026.30758</td>\n",
       "      <td>191026.509560</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>645.854171</td>\n",
       "      <td>1798.010924</td>\n",
       "      <td>9.335065</td>\n",
       "      <td>10.812186</td>\n",
       "      <td>0.891211</td>\n",
       "      <td>0.108789</td>\n",
       "      <td>3</td>\n",
       "      <td>122903.848354</td>\n",
       "      <td>122904.050334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>767.133223</td>\n",
       "      <td>9.583333</td>\n",
       "      <td>191026.55758</td>\n",
       "      <td>191026.759608</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>645.854171</td>\n",
       "      <td>1798.010924</td>\n",
       "      <td>9.335065</td>\n",
       "      <td>10.812186</td>\n",
       "      <td>0.891211</td>\n",
       "      <td>0.108789</td>\n",
       "      <td>4</td>\n",
       "      <td>122904.098354</td>\n",
       "      <td>122904.300382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>912.280287</td>\n",
       "      <td>9.833333</td>\n",
       "      <td>191026.80758</td>\n",
       "      <td>191027.009048</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>645.854171</td>\n",
       "      <td>1798.010924</td>\n",
       "      <td>9.335065</td>\n",
       "      <td>10.812186</td>\n",
       "      <td>0.891211</td>\n",
       "      <td>0.108789</td>\n",
       "      <td>5</td>\n",
       "      <td>122904.348354</td>\n",
       "      <td>122904.549821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11515</th>\n",
       "      <td>2169.780417</td>\n",
       "      <td>11.083333</td>\n",
       "      <td>194544.45757</td>\n",
       "      <td>194544.659529</td>\n",
       "      <td>12.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>645.854171</td>\n",
       "      <td>1798.010924</td>\n",
       "      <td>9.335065</td>\n",
       "      <td>10.812186</td>\n",
       "      <td>0.074206</td>\n",
       "      <td>0.925794</td>\n",
       "      <td>11516</td>\n",
       "      <td>126421.998344</td>\n",
       "      <td>126422.200303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11516</th>\n",
       "      <td>1448.154688</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>194544.70757</td>\n",
       "      <td>194544.909898</td>\n",
       "      <td>12.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>645.854171</td>\n",
       "      <td>1798.010924</td>\n",
       "      <td>9.335065</td>\n",
       "      <td>10.812186</td>\n",
       "      <td>0.074206</td>\n",
       "      <td>0.925794</td>\n",
       "      <td>11517</td>\n",
       "      <td>126422.248344</td>\n",
       "      <td>126422.450671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11517</th>\n",
       "      <td>2169.780417</td>\n",
       "      <td>11.083333</td>\n",
       "      <td>194544.95757</td>\n",
       "      <td>194545.159566</td>\n",
       "      <td>12.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>645.854171</td>\n",
       "      <td>1798.010924</td>\n",
       "      <td>9.335065</td>\n",
       "      <td>10.812186</td>\n",
       "      <td>0.074206</td>\n",
       "      <td>0.925794</td>\n",
       "      <td>11518</td>\n",
       "      <td>126422.498344</td>\n",
       "      <td>126422.700340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11518</th>\n",
       "      <td>2580.318310</td>\n",
       "      <td>11.333333</td>\n",
       "      <td>194545.20757</td>\n",
       "      <td>194545.409397</td>\n",
       "      <td>12.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>645.854171</td>\n",
       "      <td>1798.010924</td>\n",
       "      <td>9.335065</td>\n",
       "      <td>10.812186</td>\n",
       "      <td>0.074206</td>\n",
       "      <td>0.925794</td>\n",
       "      <td>11519</td>\n",
       "      <td>126422.748344</td>\n",
       "      <td>126422.950170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11519</th>\n",
       "      <td>2896.309376</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>194545.45757</td>\n",
       "      <td>194545.659437</td>\n",
       "      <td>12.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>645.854171</td>\n",
       "      <td>1798.010924</td>\n",
       "      <td>9.335065</td>\n",
       "      <td>10.812186</td>\n",
       "      <td>0.074206</td>\n",
       "      <td>0.925794</td>\n",
       "      <td>11520</td>\n",
       "      <td>126422.998344</td>\n",
       "      <td>126423.200210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11520 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       frequencies  frequencies_oct        timing  timing_offset   run  block  \\\n",
       "0       683.438005         9.416667  191025.80758  191026.009564   1.0    1.0   \n",
       "1       683.438005         9.416667  191026.05758  191026.259580   1.0    1.0   \n",
       "2       683.438005         9.416667  191026.30758  191026.509560   1.0    1.0   \n",
       "3       767.133223         9.583333  191026.55758  191026.759608   1.0    1.0   \n",
       "4       912.280287         9.833333  191026.80758  191027.009048   1.0    1.0   \n",
       "...            ...              ...           ...            ...   ...    ...   \n",
       "11515  2169.780417        11.083333  194544.45757  194544.659529  12.0   24.0   \n",
       "11516  1448.154688        10.500000  194544.70757  194544.909898  12.0   24.0   \n",
       "11517  2169.780417        11.083333  194544.95757  194545.159566  12.0   24.0   \n",
       "11518  2580.318310        11.333333  194545.20757  194545.409397  12.0   24.0   \n",
       "11519  2896.309376        11.500000  194545.45757  194545.659437  12.0   24.0   \n",
       "\n",
       "       segment  center_freq_a  center_freq_b  center_freq_a_oct  \\\n",
       "0          1.0     645.854171    1798.010924           9.335065   \n",
       "1          1.0     645.854171    1798.010924           9.335065   \n",
       "2          1.0     645.854171    1798.010924           9.335065   \n",
       "3          1.0     645.854171    1798.010924           9.335065   \n",
       "4          1.0     645.854171    1798.010924           9.335065   \n",
       "...        ...            ...            ...                ...   \n",
       "11515     10.0     645.854171    1798.010924           9.335065   \n",
       "11516     10.0     645.854171    1798.010924           9.335065   \n",
       "11517     10.0     645.854171    1798.010924           9.335065   \n",
       "11518     10.0     645.854171    1798.010924           9.335065   \n",
       "11519     10.0     645.854171    1798.010924           9.335065   \n",
       "\n",
       "       center_freq_b_oct  probability_a  probability_b  stimulus  \\\n",
       "0              10.812186       0.891211       0.108789         1   \n",
       "1              10.812186       0.891211       0.108789         2   \n",
       "2              10.812186       0.891211       0.108789         3   \n",
       "3              10.812186       0.891211       0.108789         4   \n",
       "4              10.812186       0.891211       0.108789         5   \n",
       "...                  ...            ...            ...       ...   \n",
       "11515          10.812186       0.074206       0.925794     11516   \n",
       "11516          10.812186       0.074206       0.925794     11517   \n",
       "11517          10.812186       0.074206       0.925794     11518   \n",
       "11518          10.812186       0.074206       0.925794     11519   \n",
       "11519          10.812186       0.074206       0.925794     11520   \n",
       "\n",
       "          timing_meg  timing_offset_meg  \n",
       "0      122903.348354      122903.550338  \n",
       "1      122903.598354      122903.800354  \n",
       "2      122903.848354      122904.050334  \n",
       "3      122904.098354      122904.300382  \n",
       "4      122904.348354      122904.549821  \n",
       "...              ...                ...  \n",
       "11515  126421.998344      126422.200303  \n",
       "11516  126422.248344      126422.450671  \n",
       "11517  126422.498344      126422.700340  \n",
       "11518  126422.748344      126422.950170  \n",
       "11519  126422.998344      126423.200210  \n",
       "\n",
       "[11520 rows x 16 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_beh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dd9767",
   "metadata": {},
   "source": [
    "## transformation into the time domain\n",
    "Below some function to help translate stimulus domain dataframes into timedomain ones (as would be the case for Pupil or MEG data).\n",
    "+ `map_stim_to_time` takes the timedomain and stimulus domain (*df_beh*) dataframes and uses the onset and offset times (must be in same domain) to fit stimulus number into timedomain dataframe (*df*).\n",
    "    + internally it `df.apply` a costum `_assign_stimulus` function, which np.searchsorted onset and offset timings\n",
    "    + i.e. the stimulus dataframe with stim[12345] will fit within the timedomain as [00001110022200333004440055500]\n",
    "+ `map_columns_to_time` takes the adjusted timedomain dataframe (with stimulus information) and fits collumns of interest into this new format\n",
    "    + internally it uses `df[indicator_nm].map(stimulus_to_col).fillna(0)` to fill stimulus timecourses with per stimulus information\n",
    "    + e.g. the stimulus dataframe [frequency information] stim[200 100 50 200] will fit within the timedomain as [0 0 0 0 200 200 200 0 0 100 etc.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be374f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MOVE FUNCTIONS FROM STIMULUS DOMAIN INTO TIMEDOMAIN\n",
    "## MAP 'STIMULUS' TO INDEX OF WHAT STIMULI, FOR EASY MAPPING\n",
    "\n",
    "def stim_save_segments(df_beh, groupby_nm=['block', 'segment']):\n",
    "    \"\"\"apply a new column to behavioural dataframe with segment_all - \n",
    "    indicating a continious numerical indicator of what segment we are on\"\"\"\n",
    "\n",
    "    # predefine segement all in df_beh\n",
    "    df_beh['segment_all'] = np.nan\n",
    "    \n",
    "    # loop over block and segment combinations\n",
    "    for idx, row in df_beh.groupby(groupby_nm).first().reset_index().iterrows():\n",
    "\n",
    "        # save new segment all \n",
    "        df_beh.loc[(df_beh['block'] == row['block']) & (df_beh['segment'] == row['segment']), 'segment_all'] = idx \n",
    "\n",
    "    # return the dataframe\n",
    "    return(df_beh)\n",
    "\n",
    "\n",
    "def map_stim_to_time(df, df_beh, cn_stim='stimulus', cn_ts='TIMESTAMP', \n",
    "                     beh_cn_onset='timing_meg', beh_cn_offset='timing_offset_meg'):\n",
    "    \"\"\"transform / map stimulus dataframe into the time domain\n",
    "    input: df: Pandas dataframe - time domain\n",
    "           df_beh: Pandas dataframe - stim domain\n",
    "           cn_stim: (optional) column name for stimulus indicator\n",
    "           cn_ts: (optional) column name for timestamp indicater in time df\n",
    "           beh_cn_onset: (optional) column name for onset time in beh_df\n",
    "           beh_cn_offset: (optional) column name for offset time in beh_df\n",
    "    returns adjusted time domain df (Pandas dataframe) with columns existing fetched from beh df\"\"\"\n",
    "    \n",
    "    # Apply the function to create the 'stimulus' column in df_time\n",
    "    df['stimulus'] = df['TIMESTAMP'].apply(_assign_stimulus, \n",
    "                                           df_beh=df_beh,\n",
    "                                           cn_onset=beh_cn_onset,\n",
    "                                           cn_offset=beh_cn_offset,\n",
    "                                           cn_stimulus=cn_stim)\n",
    "    # Return the dataframe\n",
    "    return(df)\n",
    "\n",
    "\n",
    "def map_columns_to_time(df, df_beh, col_to_trans, indicator_nm='stimulus'):\n",
    "    \"\"\"map columns of interest to transfer to timedomain\n",
    "    df: dataframe in timedomain\n",
    "    df_beh: dataframe in stim domain\n",
    "    col_to_trans: all columns to transfer\n",
    "    indicator_nm: (optional) indicator name - what to use for the mapping\"\"\"\n",
    "    \n",
    "    # loop over columns to transfer\n",
    "    for colnm in col_to_trans:\n",
    "\n",
    "        # create a dictionary to map 'stimulus' to all conditions I want to transfer to the other df\n",
    "        stimulus_to_col = df_beh.set_index(indicator_nm)[colnm].to_dict()\n",
    "\n",
    "        # map function to go from one to another\n",
    "        df[colnm] = df[indicator_nm].map(stimulus_to_col).fillna(0)\n",
    "        \n",
    "    # returns dataframe\n",
    "    return(df)\n",
    "\n",
    "\n",
    "def map_block_to_run(df, df_beh, run_nm='run', block_nm_beh='block', block_nm='BLOCK'):\n",
    "    \"\"\"map from blocknumber to run number in timedomain\"\"\"\n",
    "\n",
    "    # create a dictionary to map 'block' to run\n",
    "    map_block_run = df_beh.set_index(block_nm_beh)[run_nm].to_dict()\n",
    "\n",
    "    # map the df\n",
    "    df[run_nm] = df[block_nm].map(map_block_run).fillna(0).astype(int)\n",
    "\n",
    "    # return dataframe\n",
    "    return(df)\n",
    "\n",
    "\n",
    "def time_save_segments(df, df_beh,\n",
    "                       groupby_nm=['block', 'segment'],\n",
    "                       onset_nm='timing_meg',\n",
    "                       offset_nm='timing_offset_meg',\n",
    "                       timing_mm='TIMESTAMP'\n",
    "                      ):\n",
    "    \"\"\"save segments into the time domain dataframe\n",
    "    df: timedomain dataframe\n",
    "    df_beh: stimdomain dataframe\n",
    "    groupby_nm: (optional) list of names to groupby\n",
    "    onset_nm: (optional) what to use as onset timings - in same timeframe\n",
    "    offset_nm: (optional) what to use as offset timings - in same timeframe\n",
    "    timing_nm: (optional) time indicator in original dataframe\"\"\"\n",
    "\n",
    "    # get dataframe of onset and offset timings only\n",
    "    onset_df = df_beh.groupby(groupby_nm).first()[onset_nm].reset_index()\n",
    "    offset_df = df_beh.groupby(groupby_nm).last()[offset_nm].reset_index()\n",
    "\n",
    "    # predefine all new columns in our timedomain dataframe\n",
    "    df['block'] = np.nan\n",
    "    df['segment'] = np.nan\n",
    "    df['segment_all'] = np.nan\n",
    "\n",
    "    # loop over all index (combinations)\n",
    "    for idx, row in onset_df.iterrows():\n",
    "\n",
    "        # get start and endtime of groupby section\n",
    "        cur_onset = onset_df[onset_nm].iloc[idx]\n",
    "        cur_offset = offset_df[offset_nm].iloc[idx]\n",
    "\n",
    "        # map to OG dataframe\n",
    "        df.loc[(df[timing_mm] >= cur_onset) & (df[timing_mm] <= cur_offset), 'segment'] = onset_df['segment'].iloc[idx]\n",
    "\n",
    "        # save per segment indicator\n",
    "        df.loc[(df[timing_mm] >= cur_onset) & (df[timing_mm] <= cur_offset), 'segment_all'] = idx\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "\n",
    "def time_save_onoff(df, onoff_nm='onoff', indicator='stimulus'):\n",
    "    \"\"\"save onoff value (bool), based on indicator value\"\"\"\n",
    "\n",
    "    # predefine\n",
    "    df['onoff'] = 0\n",
    "    # take wherever there is any stimulus - set to 1\n",
    "    df.loc[(df['stimulus'] > 0), 'onoff'] = 1\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "\n",
    "# create a function to assign stimuli based on timing\n",
    "def _assign_stimulus(timing, \n",
    "                     df_beh, \n",
    "                     cn_onset='timing_meg', \n",
    "                     cn_offset='timing_offset_meg',\n",
    "                     cn_stimulus='stimulus'):\n",
    "    \"\"\"pandas apply function to get stimuli into the time domain\n",
    "    input df_beh, cn_onset (optional columnname of onset time),\n",
    "    cn_offset (optional columnname of offset time), cn_stimulus (optional columnname of stimulus)\"\"\"\n",
    "    idx = np.searchsorted(df_beh[cn_onset], timing)\n",
    "    if idx == 0 or timing >= df_beh[cn_offset].iloc[idx - 1]:\n",
    "        return 0\n",
    "    return df_beh[cn_stimulus].iloc[idx - 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b41dab",
   "metadata": {},
   "source": [
    "### Example of use\n",
    "\n",
    "Below some examples of use. Note that these examples were copied from my MEG script and are not directly translateable, however they should give you a gist of how the approach works.\n",
    "\n",
    "- I also left in the mapping of segment data, to show that these functions should be relatively robust in multiple senarios\n",
    "where `map_columns_to_time(df, df_beh, col_to_trans, indicator_nm='stimulus ')` will fit columns based on stimulus as indicator\n",
    "`map_columns_to_time(df, df_beh, col_to_trans, indicator_nm='segment_all')` will fit columns based on segment (but can also be used for blocks etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34dd2ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map stimulus indexing to timedomain (123 > 00011100222000333)\n",
    "df = map_stim_to_time(df, df_beh, cn_stim='stimulus', cn_ts='TIMESTAMP', \n",
    "                     beh_cn_onset='timing_meg', beh_cn_offset='timing_offset_meg')\n",
    "\n",
    "# get columns of interest to transfer and apply stim specific mapping\n",
    "col_to_trans = ['frequencies', 'frequencies_oct', 'forward_adapation', 'forward_adapted_activation', 'surprisal', 'pred_prob']\n",
    "df = map_columns_to_time(df, df_beh, col_to_trans)\n",
    "\n",
    "# save segment and segment all in timedomain and segment all in main stimulus domain dataframe\n",
    "df = time_save_segments(df, df_beh)\n",
    "df_beh = stim_save_segments(df_beh)\n",
    "\n",
    "# use the blocknumber to runnumber pairing in stimulus domain to map block to run in time domain\n",
    "df = map_block_to_run(df, df_beh)\n",
    "\n",
    "# map segment specific data onto current segment\n",
    "col_to_trans = ['center_freq_a', 'center_freq_b', 'center_freq_a_oct', 'center_freq_b_oct', 'probability_a', 'probability_b']\n",
    "df = map_columns_to_time(df, df_beh, col_to_trans, indicator_nm='segment_all')\n",
    "\n",
    "# save onoff inside timing dataframe\n",
    "df = time_save_onoff(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
